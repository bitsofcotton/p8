# p8
The combination of the predictions.

# Description
If we take {0,1,2}-valued function as maximum variables, we get up to 16 depth tri-color tree.
So we decompose input stream to such structures with inverse with context-depend compressions, then, predict with leaf is pseudo Lebesgue measurable continuous ones.

# Why we need this
Once p0 has a almost complete prediction on the stream, the selection of the hypothesis continuous had be switched we think (or, our terminal is infected).
So if this is the switch from the world itself, we think this can be the second one with using the world compression observation context if they're variable.

# The hypothesis this condition
We make the hypothesis there's no tanglement on variable to variable, so they're completely separable, however, the value itself has some of the representations on the number as a context.

Also, we make the hypothesis represented leaf of them are enough Lebesgue measurable continuous ones.

# Don't implement now
Because p1+p0 has half compatible to this structure, and this can break some of the continuous condition on the world, we don't implement this structure.

# Ongoing our end of the implementation
This is the p\*-series our end of the implementation if randtools is enough meaningful.

# Might be our end of the implementation
Without making hypothesis on the internal status on function made stream, there's variables on the hypothesis {1,x,f(x),status}, in some condition this is reduced into {x,f(x)}.
The latter one we treat with the p0, p1 for maximum dimension on the stream of the matrix rank.
The former one we treat also with p4, p7 adding to p0, p1. In the matrix rank meaning.

So to continue p\*-series is to enname some series of the vector set on prediction, but these are max rank, we end with this.

Treating the internal status on somewhat larger separated one and to make the hypothesis the structure is simple enough, the p3 is our end of the implementation. So if we implement p3 and there's some high complexity structure on their internal states, p3 will be the better choice for us, however, in {x,f(x)} meaning, they should compatible to our p0, p1.

So this is our end, and open for implementation.
Also, I'm doubting our companion computer structure's consistency, this is also opposite site open for implementation.

# Relation between our p-... and deep learning predictors
From our view of the stream prediction, deep learning predictors are similar to p3 but is focusing copying complex enough structure with continuous enough functions with multi layered PDE they represents one PDE.

However, p3 with maximum complexity with simple enough structure could rarely but some effect on fights with them.
This is because v2v tanglement on all of the variables causes 3 variable structures, and they have an upper bound on function numbers.
This shows upper bound of configurable parameters if we only separate in/out/status.

However, even in such case, deep learning can learn such structures, so they only have slight meaning.

Also, it is almost equivalent such learning to gather variables with compression, so p3 is usefull only the case we need the descriptionability on the model.

# Tips on entropy between the function need and the data stream have
p8 uses the compression/decompression method to exclude complexity into last observation insists as a intent.
So to avoid intension based slip, we should use the parameters pull back from the data stream they causes entropy the function exports from same input.
So this is done by recursion of the p1 part as a coefficient of the each, this describes the 4 dimension slices from the data stream per each, they causes increase of the entropy the function have causes last observation don't need to do much thing and don't effect to the prediction itself, since we compress/decompress with observation theirselves, with fixed vector on tan \<a,x\> instead of the variable parameter configured vectors.

So the matter is, how many layers we need to describe with p8 method to predict original stream better well, however, this might be depend on the entropy the function have, in another words, the ratio with the function layer number vs. the viewpoint the original data stream have. If #f countup glitch is effective to the viewpoint number the original data stream have, the layer number itself also have the upper bound for any of the data stream, however, data stream can have any stream when we're stacking copying layer. One possibility is there to have such stack causes equivalent to another low number of the stack. We don't analyse this now.

In addition to this, bitsofcotton/NAND:Readme.md:General Tips on any functions: describes P01 in p1 class is enough because first order optimization returns 3x4 of the parameters, either, P1 on P1 also better with because they returns 4x4 parameters at leafs. These makes sense if the optimization (taking invariant) can pull back full set of the input stream information without any of the loss (as a observation on the stream and us). However, even in this case, the result can have to be broken if some of the uncontinuous part appears on next step point. So p8 have intention to exclude such uncontinuous part into prime factor's Factor matrix observation with much sequences on input. However, the stream can have the next step flipped stream from sone of the predictor in any which way to exclude such uncontinuous part.

# Tips on better treating output size concern with function entropy
If we treat original input/output as shrinking/enlarging them by a factor matrix based method and correct matrix chain, they causes observation/(switch?) operations.
However, if the pure function treats input data stream as single layerd on some of the block operations after optimizing function, this means the function doesn't attaches special meanings on the data, treat each data as equal on processing in each multiple stacked layer, the function entropy has upper bound. This causes output stream complexity cannot get better entropy from input stream.

So to have output stream much of the entropy they've caused by many of the viewpoint assisted by somme of the internal status the function makes, the (pure) function need to treat some part of the data heavy weight, some another part of the data light weight, on computation complexity nor algorithm loop number.
One of the clue we have is from bitsofcotton/randtools:tips AG, so some of the topological index number can have some another complexity results (does not carefully tested).

# Tips on compatibility to p0, p1, p2.
Taking p1/pp3.cc quads to original stream causes (aleph_1)^(aleph_1) combination into null set also the result can be cutted {RNG, continuity (self similarity), combination} causes {const.} also second or quad order observation needs to in some of the continuity. A continuity and self similarity is similar in aleph_1 but is different in aleph_0. This is valid if the base dimension of continuity is 3 and it's up to aleph_2 each of them can be up to omega. Since we cannot estimate aleph_3 nor (aleph_1)^(aleph_1) their selves nor, aleph_k counting can have tanglement greater than {0,1,2}, so RNG also cutted by our perspective but not in real stream might be appeared real self similarity.

So after doing pp3 thirds, p0/p0.cc causes some of the prediction they can be generic one.

Also, if we fight with the functions only up to aleph_0 (discrete points with no upper cardinal), doing pp3 seconds also causes (aleph_0)^(aleph_0) vanish into null set also {RNG, combination} into {const.} however either some signs changes in practical with lack of the internal states we can feed into predictor.

The p2/p2prng.cc causes better compatible with this hypothesis with pp3n thirds.
However, only one column another words single pp3 isn't with smaller length.
This might because of the shortage of input internal bit estimation, so they can expose some initial internal bits to the predictors.

