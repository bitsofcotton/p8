# p8
The combination of the predictions.

# Description
If we take {0,1,2}-valued function as maximum variables, we get up to 16 depth tri-color tree.
So we decompose input stream to such structures with inverse with context-depend compressions, then, predict with leaf is pseudo Lebesgue measurable continuous ones.

# Why we need this
Once p0 has a almost complete prediction on the stream, the selection of the hypothesis continuous had be switched we think (or, our terminal is infected).
So if this is the switch from the world itself, we think this can be the second one with using the world compression observation context if they're variable.

# The hypothesis this condition
We make the hypothesis there's no tanglement on variable to variable, so they're completely separable, however, the value itself has some of the representations on the number as a context.

Also, we make the hypothesis represented leaf of them are enough Lebesgue measurable continuous ones.

# Don't implement now
Because p1+p0 has half compatible to this structure, and this can break some of the continuous condition on the world, we don't implement this structure.

# Ongoing our end of the implementation
This is the p\*-series our end of the implementation if randtools is enough meaningful.

# Might be our end of the implementation
Without making hypothesis on the internal status on function made stream, there's variables on the hypothesis {1,x,f(x),status}, in some condition this is reduced into {x,f(x)}.
The latter one we treat with the p0, p1 for maximum dimension on the stream of the matrix rank.
The former one we treat also with p4, p7 adding to p0, p1. In the matrix rank meaning.

So to continue p\*-series is to enname some series of the vector set on prediction, but these are max rank, we end with this.

Treating the internal status on somewhat larger separated one and to make the hypothesis the structure is simple enough, the p3 is our end of the implementation. So if we implement p3 and there's some high complexity structure on their internal states, p3 will be the better choice for us, however, in {x,f(x)} meaning, they should compatible to our p0, p1.

So this is our end, and open for implementation.
Also, I'm doubting our companion computer structure's consistency, this is also opposite site open for implementation.

# Relation between our p-... and deep learning predictors
From our view of the stream prediction, deep learning predictors are similar to p3 but is focusing copying complex enough structure with continuous enough functions with multi layered PDE they represents one PDE.

However, p3 with maximum complexity with simple enough structure could rarely but some effect on fights with them.
This is because v2v tanglement on all of the variables causes 3 variable structures, and they have an upper bound on function numbers.
This shows upper bound of configurable parameters if we only separate in/out/status.

However, even in such case, deep learning can learn such structures, so they only have slight meaning.

Also, it is almost equivalent such learning to gather variables with compression, so p3 is usefull only the case we need the descriptionability on the model.

# Tips on entropy between the function need and the data stream have
p8 uses the compression/decompression method to exclude complexity into last observation insists as a intent.
So to avoid intension based slip, we should use the parameters pull back from the data stream they causes entropy the function exports from same input.
So this is done by recursion of the p1 part as a coefficient of the each, this describes the 4 dimension slices from the data stream per each, they causes increase of the entropy the function have causes last observation don't need to do much thing and don't effect to the prediction itself, since we compress/decompress with observation theirselves, with fixed vector on tan \<a,x\> instead of the variable parameter configured vectors.

So the matter is, how many layers we need to describe with p8 method to predict original stream better well, however, this might be depend on the entropy the function have, in another words, the ratio with the function layer number vs. the viewpoint the original data stream have. If #f countup glitch is effective to the viewpoint number the original data stream have, the layer number itself also have the upper bound for any of the data stream, however, data stream can have any stream when we're stacking copying layer. One possibility is there to have such stack causes equivalent to another low number of the stack. We don't analyse this now.

